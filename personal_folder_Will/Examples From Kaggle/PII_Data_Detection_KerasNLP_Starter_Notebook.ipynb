{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 66653,
          "databundleVersionId": 7500999,
          "sourceType": "competition"
        },
        {
          "sourceId": 7526248,
          "sourceType": "datasetVersion",
          "datasetId": 4308295
        },
        {
          "sourceId": 7626226,
          "sourceType": "datasetVersion",
          "datasetId": 4442914
        },
        {
          "sourceId": 6064,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 4685
        },
        {
          "sourceId": 6065,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 4686
        },
        {
          "sourceId": 6068,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 4689
        },
        {
          "sourceId": 6069,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 4690
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "PII Data Detection: KerasNLP Starter Notebook",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'pii-detection-removal-from-educational-data:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F66653%2F7500999%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240303%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240303T201340Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D16e667fb2f7e29d3a5c895a1e4dcc008960d5a7e26e7e2ee76eff4cc0f3fabe1c964033417604c6749798d7181e4acc448459c9ef0baec3aa5dca302f95123e22588e90be6212264537d61846c0af308bd58aeda828b11764cb370517d76cba76894be4f400c7c9236c80826c900a6fa9904d02b030b611b7a1dba4147fcead7e39e9afdce919195984dce18557beef957880b4d3587c8e06d0930c10406806588f1bd10ce621d10c2480660dd1c211b8eaf0722d9528cf2021807a85b4c3f0705299fe3daa5fbd389de5a0fc4a20b9878d78609a9a148fa8ab0438c6488be54795232332c260b5eb6458b81a135fe07133ff544e4c47a805589f9633c16731e,kerasv3-lib-ds:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4308295%2F7526248%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240303%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240303T201340Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D557081090a50b77f3ef1d78535d51a3860dab94da40d7edb5898b9762a128bbbebd8195beaa609deecb130ba1c7f63ee2ca198c57e07013fbe19f49ce8215641aed289306400756989ed1c50b547e80aaf86cec582699e88d8dbd079a12278e2ebb3bcc0c33d9c1b23a8e3b7063e664cbf0996098861665b4d800cd852057a5dc57def3efa5b6a90b9fbac81139a2a4cf8953ce9c1224b80dc5f3d593b17dc6983db02e8b3a53c2ad1583dcf4dab4250fdcd020edfb30d69199eb24367c385b4d897828c9f89f8f070d42ab96d3508bce834159802500b288dd8d100dc5b524bf7f6a36f6bcce3ae301c30f680e03eeeb72871d1305fe8b866d705b79118d77b,pii-data-detection-ckpt-ds:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4442914%2F7626226%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240303%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240303T201340Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9e5db3a2cc5bdab35fc722bc570039e8dd6569a8d4e688347905a4059d1f806cbf5789fed470ad024f8b869b6e72a3b0b568e72ce56d18859af53b70b2b0f9b4d278c25982ee75572ab3e9372a224b70b3d19c92dec38e47bb2c06ff84e7186ded00562c587a0547d8c3b65dcb8b4fbb3621e55cd5cd88c59c4f2fe27d5d9c49d530bbd1dd33d09f7758479c2780d93fe9e51b8dd485af968a75dc9b720cd08dad7ab760db1d541f363e1180786351845396afea804bd5626b2f7c73492388a761e8b73ad7fe8d6f1e40e689998f730ce342299c770f4b132f5c129bb82af2e895a67f91573346503c0cca5d3cdfbe6c196a109355465981c6ffe10ec8b28cf7,deberta_v3/keras/deberta_v3_small_en/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F4685%2F6064%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240303%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240303T201340Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D52dec85a46b40394886501c9cc7afef653afc1f0d0106601f5f6a2e0b3428c45c33ba67910130def60575aac894d03e952ae8b58d30fa7052e4d513d7f0bc12e90f093cf9f1dc2d15b202996db4e5ad3c7760d90de1cb51d6d764b7a1693559bb06a189a4244abde9642187a7692ea3b48c0c8b9885f877ac4bb24b82ef1b7d01b4e552d2d3845498ac5337498c097f283ad4c900a9acb8ce6197d0e3e1e616831ba0aec92fc4c9a57ede87b3fbfe597f9d611a195a581794bd4c39a10b2983ca3aaeafde94a83e18cda29c79745de3f4fc711751274529e7129dbaeacb5f0ea62b1fb7ad187aecdad9feda6969f4b063e3854fe047e13b88a0cafaa63c694b3,deberta_v3/keras/deberta_v3_base_en/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F4686%2F6065%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240303%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240303T201340Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9d0205744fb48f58d75ea811853cf8a25e760def47c8278123d0103e7469ea721c65eafe711d1512c92206d59fae4aee5ec5078b17c0c3bcf52cd6d184782fcca3fd984b3dce4431ecf3a894a520c8d4c97070d215938c0992b8dd9ac9d7a27e985e98e4f501c6aca8df71f644738ab48d90977f3b082d53e6a047a3472517cc04be2fc60dc29b3b97ee8b6a210a62e1189f0610edcbc0a4e4861ca6ab917f8b91552828a80f712f006deb0f34b58f16987ef615167573a81a5feda7af1c6ca5a951e3485e96928ebc05f738f005207d3628d0b101071a8c8298c12b5fa136b1f4e337c45aa8493d0e9f57d96b3ff7e6ebc461f452464f8857c04f206a53d9e8,distil_bert/keras/distil_bert_base_en_uncased/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F4689%2F6068%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240303%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240303T201340Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D55fa4a14e4b53d99bd88dfcb0a4ed2737859cb068c31db9a44d7614a74939381ab5ad9b4b012f39e79870c5699c5bbef7ae2b71e725c2282e092ff421a227eb643b8391ea4b82657b4c1242d0b9757188dd657fa87d953b886b7e9cd709b506bc0e112f03acd1e8773c44a7442e17db71aa65a059a717c74f1be105973808b648e79d4360a3f6237c6a6858f14f873c2d0e5c3eca4c22d9b880562bcd20489ff633fdd5737a6672ac0416073160a4beca41d6606d73d71ec6bf9e51b0c156eba13a432f631c91100f5bfe78412c0a913db6100d09f80a2b3d7529468cdf6298428223a54523703ebd56342f5a5e444b1d0ff0da7c09f346b66ce56d86e0aceb1,distil_bert/keras/distil_bert_base_en/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F4690%2F6069%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240303%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240303T201340Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D88f0c2636f6e80aa74ea9ed3365c379f73d6deca1237b2c8bf56f3d9d6e9b8700a7e26a14ff79a65cbf963d26062e336ab279275adaf6d66a97e98746e3b76aea497fb37c264692818cc1c31aeed2f08177f10aab6e613673acb4bcbbfb80a2ff5044596e01ba34ab6c9fe2f46c4a5e8179b49f062c80fd1165d35c002978c97231a6b7f0712bee680b386bce4da65774e742c510aa8f7e76887bed4116c58726e28e6cee12a8e5859cbe14610761a78de63bde7242651d575d63398d30c4409de7b040bd86d95bedccb86931a2129dcefa64a9156001c558028be076ea7f363613aab3abcd2a29b1427f7049d512757d63aa627127a59bb3cb6115ca498f235'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "HKlq6HuI82eg"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\n",
        "This starter notebook is provided by the Keras team.</center>"
      ],
      "metadata": {
        "id": "lkQc-ktG82ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PII Data Detection with [KerasNLP](https://github.com/keras-team/keras-nlp) and [Keras](https://github.com/keras-team/keras)\n",
        "\n",
        "> The objective of this competition is to detect and remove personally identifiable information (PII) from student writing.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://i.ibb.co/3stPB0t/pii-data-detection.jpg\" alt=\"PII Data Detection\">\n",
        "</div>\n",
        "\n",
        "The task of this competition falls under **Token Classification** (not Text Classification!), sometimes known as **Named Entity Recognition (NER)**. This notebook guides you through performing this task from scratch for the competition. Implementing from scratch is a unique feature of this notebook, as most public notebooks use **HuggingFace** to handle modeling and data processing, which performs many tasks under the hood. One may have to look deeper into the repository to understand what is happening inside. In contrast, this notebook goes step by step, showing you exactly how Token Classification works. A cherry on top: this notebook leverages **Mixed Precision** and **Distributed (multi-GPU)** Training/Inference to turbocharge performance!\n",
        "\n",
        "<u>Fun fact</u>: This notebook is backend-agnostic, supporting TensorFlow, PyTorch, and JAX. Utilizing KerasNLP and Keras allows us to choose our preferred backend. Explore more details on [Keras](https://keras.io/keras_3/).\n",
        "\n",
        "In this notebook, you will learn how to:\n",
        "\n",
        "- Design a data pipeline for token classification.\n",
        "- Create a model for token classification with KerasNLP.\n",
        "- Load the data efficiently using [`tf.data`](https://www.tensorflow.org/guide/data).\n",
        "- Perform Mixed Precision and Distributed Training/Inference with Keras 3.\n",
        "- Make submission on test data.\n",
        "\n",
        "**Note**: For a more in-depth understanding of KerasNLP, refer to the [KerasNLP guides](https://keras.io/keras_nlp/).\n"
      ],
      "metadata": {
        "id": "qBWbBOm382el"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ›  | Install Libraries  \n",
        "\n",
        "Since internet access is **disabled** during inference, we cannot install libraries in the usual `!pip install <lib_name>` manner. Instead, we need to install libraries from local files. In the following cell, we will install libraries from our local files. The installation code stays very similar - we just use the `filepath` instead of the `filename` of the library. So now the code is `!pip install <local_filepath>`.\n",
        "\n",
        "> The `filepath` of these local libraries look quite complicated, but don't be intimidated! Also `--no-deps` argument ensures that we are not installing any additional libraries."
      ],
      "metadata": {
        "id": "-cBAIU5w82el"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q /kaggle/input/kerasv3-lib-ds/tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-deps\n",
        "!pip install -q /kaggle/input/kerasv3-lib-ds/keras-3.0.4-py3-none-any.whl --no-deps"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-02-16T07:38:19.323771Z",
          "iopub.execute_input": "2024-02-16T07:38:19.324124Z",
          "iopub.status.idle": "2024-02-16T07:39:51.496906Z",
          "shell.execute_reply.started": "2024-02-16T07:38:19.324095Z",
          "shell.execute_reply": "2024-02-16T07:39:51.495882Z"
        },
        "trusted": true,
        "id": "gUG38sit82el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“š | Import Libraries"
      ],
      "metadata": {
        "id": "0LF4rz-x82em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\" # # you can also use tensorflow or torch\n",
        "\n",
        "import keras\n",
        "import keras_nlp\n",
        "from keras import ops\n",
        "import tensorflow as tf\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-02-16T07:39:51.49883Z",
          "iopub.execute_input": "2024-02-16T07:39:51.499122Z",
          "iopub.status.idle": "2024-02-16T07:39:59.808234Z",
          "shell.execute_reply.started": "2024-02-16T07:39:51.499095Z",
          "shell.execute_reply": "2024-02-16T07:39:59.807184Z"
        },
        "trusted": true,
        "id": "vQWUW68l82em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Versions"
      ],
      "metadata": {
        "id": "coklBIgr82em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"Keras:\", keras.__version__)\n",
        "print(\"KerasNLP:\", keras_nlp.__version__)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-02-16T07:39:59.809514Z",
          "iopub.execute_input": "2024-02-16T07:39:59.810099Z",
          "iopub.status.idle": "2024-02-16T07:39:59.814958Z",
          "shell.execute_reply.started": "2024-02-16T07:39:59.810072Z",
          "shell.execute_reply": "2024-02-16T07:39:59.814113Z"
        },
        "trusted": true,
        "id": "pwvSEGYh82em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âš™ï¸ | Configuration"
      ],
      "metadata": {
        "id": "vr6zf60582em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    seed = 42\n",
        "    preset = \"deberta_v3_small_en\" # name of pretrained backbone\n",
        "    train_seq_len = 1024 # max size of input sequence for training\n",
        "    train_batch_size = 2 * 8 # size of the input batch in training, x 2 as two GPUs\n",
        "    infer_seq_len = 2000 # max size of input sequence for inference\n",
        "    infer_batch_size = 2 * 2 # size of the input batch in inference, x 2 as two GPUs\n",
        "    epochs = 6 # number of epochs to train\n",
        "    lr_mode = \"exp\" # lr scheduler mode from one of \"cos\", \"step\", \"exp\"\n",
        "\n",
        "    labels = [\"B-EMAIL\", \"B-ID_NUM\", \"B-NAME_STUDENT\", \"B-PHONE_NUM\",\n",
        "              \"B-STREET_ADDRESS\", \"B-URL_PERSONAL\", \"B-USERNAME\",\n",
        "              \"I-ID_NUM\", \"I-NAME_STUDENT\", \"I-PHONE_NUM\",\n",
        "              \"I-STREET_ADDRESS\",\"I-URL_PERSONAL\",\"O\"]\n",
        "    id2label = dict(enumerate(labels)) # integer label to BIO format label mapping\n",
        "    label2id = {v:k for k,v in id2label.items()} # BIO format label to integer label mapping\n",
        "    num_labels = len(labels) # number of PII (NER) tags\n",
        "\n",
        "    train = True # whether to train or use already trained ckpt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:39:59.81621Z",
          "iopub.execute_input": "2024-02-16T07:39:59.816507Z",
          "iopub.status.idle": "2024-02-16T07:39:59.852304Z",
          "shell.execute_reply.started": "2024-02-16T07:39:59.816482Z",
          "shell.execute_reply": "2024-02-16T07:39:59.851285Z"
        },
        "trusted": true,
        "id": "80lnvjyv82em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# â™»ï¸ | Reproducibility\n",
        "Sets value for random seed to produce similar result in each run."
      ],
      "metadata": {
        "id": "4tM_6_8s82em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.set_random_seed(CFG.seed)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:39:59.854828Z",
          "iopub.execute_input": "2024-02-16T07:39:59.855201Z",
          "iopub.status.idle": "2024-02-16T07:39:59.863397Z",
          "shell.execute_reply.started": "2024-02-16T07:39:59.855176Z",
          "shell.execute_reply": "2024-02-16T07:39:59.862501Z"
        },
        "trusted": true,
        "id": "zfXeoEfh82eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš€ | Distributed Training / Inference\n",
        "\n",
        "In this notebook, we will also use the `Data Parallel` strategy for **Distributed Training/Inference**. This means that the model weights will be replicated across all devices, and each device will process a portion of the input data.\n",
        "\n",
        "> **Note**: Currently, `DataParallel` is implemented on the JAX backend, so for TensorFlow and PyTorch backends, we can only use a single GPU."
      ],
      "metadata": {
        "id": "7iwnyEUW82ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get devices default \"gpu\" or \"tpu\"\n",
        "devices = keras.distribution.list_devices()\n",
        "print(\"Device:\", devices)\n",
        "\n",
        "if len(devices) > 1:\n",
        "    # Data parallelism\n",
        "    data_parallel = keras.distribution.DataParallel(devices=devices)\n",
        "\n",
        "    # Set the global distribution.\n",
        "    keras.distribution.set_distribution(data_parallel)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:39:59.864448Z",
          "iopub.execute_input": "2024-02-16T07:39:59.864738Z",
          "iopub.status.idle": "2024-02-16T07:40:00.501687Z",
          "shell.execute_reply.started": "2024-02-16T07:39:59.864715Z",
          "shell.execute_reply": "2024-02-16T07:40:00.500819Z"
        },
        "trusted": true,
        "id": "GH4hJi8O82ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§® | Mixed Precision\n",
        "\n",
        "To enable larger batch sizes and faster training, we'll utilize `mixed_precision` in this notebook. In Keras, this can be achieved with just **one line of code**, as shown below."
      ],
      "metadata": {
        "id": "ixc_JBFK82ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:00.502918Z",
          "iopub.execute_input": "2024-02-16T07:40:00.503191Z",
          "iopub.status.idle": "2024-02-16T07:40:00.507333Z",
          "shell.execute_reply.started": "2024-02-16T07:40:00.503166Z",
          "shell.execute_reply": "2024-02-16T07:40:00.50634Z"
        },
        "trusted": true,
        "id": "IpAjZ1X782ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“ | Dataset Path"
      ],
      "metadata": {
        "id": "ypiTpOFH82ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/kaggle/input/pii-detection-removal-from-educational-data\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:00.508624Z",
          "iopub.execute_input": "2024-02-16T07:40:00.509255Z",
          "iopub.status.idle": "2024-02-16T07:40:00.517509Z",
          "shell.execute_reply.started": "2024-02-16T07:40:00.509222Z",
          "shell.execute_reply": "2024-02-16T07:40:00.516809Z"
        },
        "trusted": true,
        "id": "9tMr-JnU82ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“– | Meta Data\n",
        "\n",
        "The competition dataset contains ~$22,000$ student essays where $70\\%$ essays are reserved for **testing**, leaving $30\\%$ for **training** and **validation**.\n",
        "\n",
        "Sure, here's the modified markdown with an example of the BIO format label:\n",
        "\n",
        "**Data Overview:**\n",
        "\n",
        "* All essays were written in response to the **same prompt**, applying course material to a real-world problem.\n",
        "* The dataset includes **7 types of PII**: `NAME_STUDENT`, `EMAIL`, `USERNAME`, `ID_NUM`, `PHONE_NUM`, `URL_PERSONAL`, `STREET_ADDRESS`.\n",
        "* Labels are given in **BIO (Beginning, Inner, Outer)** format.\n",
        "\n",
        "**Example of BIO format label:**\n",
        "\n",
        "Let's consider a sentence: `\"The email address of Michael jordan is mjordan@nba.com\"`. In BIO format, the labels for the personally identifiable information (PII) would be annotated as follows:\n",
        "\n",
        "| **Word** | The | email | address | of | Michael | Jordan | is | mjordan@nba.com |\n",
        "|----------|-----|-------|---------|----|---------|--------|----|----------------|\n",
        "| **Label** | O   | O     | O       | O  | B-NAME_STUDENT | I-NAME_STUDENT | O  | B-EMAIL        |\n",
        "\n",
        "In the example above, `B-` indicates the beginning of an PII, `I-` indicates an inner part of a multi-token PII, and `O` indicates tokens that do not belong to any PII.\n",
        "\n",
        "**Data Format:**\n",
        "\n",
        "* The train/test data is stored in `{test|train}.json` files.\n",
        "* Each json file has:\n",
        "    * `document`: unique ID (integer)\n",
        "    * `full_text`: essay content (string)\n",
        "    * `tokens`: individual words in the essay (list of strings)\n",
        "    * `labels` (training data only): BIO labels for each token (list of strings)"
      ],
      "metadata": {
        "id": "V0O9ehrI82ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Valid data\n",
        "data = json.load(open(f\"{BASE_PATH}/train.json\"))\n",
        "\n",
        "# Initialize empty arrays\n",
        "words = np.empty(len(data), dtype=object)\n",
        "labels = np.empty(len(data), dtype=object)\n",
        "\n",
        "# Fill the arrays\n",
        "for i, x in tqdm(enumerate(data), total=len(data)):\n",
        "    words[i] = np.array(x[\"tokens\"])\n",
        "    labels[i] = np.array([CFG.label2id[label] for label in x[\"labels\"]])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:00.518633Z",
          "iopub.execute_input": "2024-02-16T07:40:00.519372Z",
          "iopub.status.idle": "2024-02-16T07:40:06.057263Z",
          "shell.execute_reply.started": "2024-02-16T07:40:00.519339Z",
          "shell.execute_reply": "2024-02-16T07:40:06.056319Z"
        },
        "trusted": true,
        "id": "IIo6IhWJ82ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Š | Exploratory Data Analysis\n",
        "\n",
        "From the following label distribution plot, it is evident that there is a significant **class imbalance** between PII tags. This could be a key area for improvement where **external datasets** and **augmentations** could play a pivotal role."
      ],
      "metadata": {
        "id": "xELslXtX82ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique labels and their frequency\n",
        "all_labels = np.array([x for label in labels for x in label])\n",
        "unique_labels, label_counts = np.unique(all_labels, return_counts=True)\n",
        "\n",
        "# Plotting\n",
        "fig = go.Figure(data=go.Bar(x=CFG.labels, y=label_counts))\n",
        "fig.update_layout(\n",
        "    title=\"Label Distribution\",\n",
        "    xaxis_title=\"Labels\",\n",
        "    yaxis_title=\"Count\",\n",
        "    yaxis_type=\"log\",\n",
        ")\n",
        "\n",
        "fig.update_traces(text=label_counts, textposition=\"outside\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:06.058598Z",
          "iopub.execute_input": "2024-02-16T07:40:06.059001Z",
          "iopub.status.idle": "2024-02-16T07:40:07.537372Z",
          "shell.execute_reply.started": "2024-02-16T07:40:06.058974Z",
          "shell.execute_reply": "2024-02-16T07:40:07.536506Z"
        },
        "trusted": true,
        "id": "XIg76Iuy82ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”ª | Data Split\n",
        "\n",
        "In the following code snippet, we will split the dataset into training and testing subsets using an `80%-20%` ratio."
      ],
      "metadata": {
        "id": "aDbfs_sU82eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and testing sets\n",
        "train_words, valid_words, train_labels, valid_labels = train_test_split(\n",
        "    words, labels, test_size=0.2, random_state=CFG.seed\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:07.538675Z",
          "iopub.execute_input": "2024-02-16T07:40:07.538958Z",
          "iopub.status.idle": "2024-02-16T07:40:07.545492Z",
          "shell.execute_reply.started": "2024-02-16T07:40:07.538933Z",
          "shell.execute_reply": "2024-02-16T07:40:07.544538Z"
        },
        "trusted": true,
        "id": "dMPosOpv82eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ½ï¸ | Pre-Processing\n",
        "\n",
        "Initially, raw text data is quite complex and challenging for modeling due to its high dimensionality. We simplify this complexity by converting text into words then more manageable set of tokens using `tokenizers`. For example, transforming the sentence `\"The quick brown fox\"` into tokens like `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]` helps us break down the text effectively. Then, since models can't directly process strings, they are converted into integers, like `[10, 23, 40, 51, 90, 84]`. Additionally, many models require special tokens and additional tensors to understand input better. A `preprocessing` layer helps with this by adding these special tokens, which aid in separating input and identifying padding, among other tasks.\n",
        "\n",
        "You can explore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n",
        "- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)\n",
        "- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)"
      ],
      "metadata": {
        "id": "3p2ILpkm82eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To convert string input or list of strings input to numerical tokens\n",
        "tokenizer = keras_nlp.models.DebertaV3Tokenizer.from_preset(\n",
        "    CFG.preset,\n",
        ")\n",
        "\n",
        "# Preprocessing layer to add spetical tokens: [CLS], [SEP], [PAD]\n",
        "packer = keras_nlp.layers.MultiSegmentPacker(\n",
        "    start_value=tokenizer.cls_token_id,\n",
        "    end_value=tokenizer.sep_token_id,\n",
        "    sequence_length=10,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:07.54673Z",
          "iopub.execute_input": "2024-02-16T07:40:07.546997Z",
          "iopub.status.idle": "2024-02-16T07:40:08.998705Z",
          "shell.execute_reply.started": "2024-02-16T07:40:07.546975Z",
          "shell.execute_reply": "2024-02-16T07:40:08.997878Z"
        },
        "trusted": true,
        "id": "UvdYQPCq82eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer in Action\n",
        "\n",
        "The following code shows the effects of `DebertaV3Tokenizer`. We can see that the word `[\"reflexion\"]` has been divided into `[\"â–reflex\", \"ion\"]` tokens. Therefore, for similar cases, it's necessary to align labels of tokens to labels of words."
      ],
      "metadata": {
        "id": "V9vWMo2D82eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_words = words[0][:5]\n",
        "sample_tokens_int = [\n",
        "    token.tolist() for word in sample_words for token in tokenizer(word)\n",
        "]\n",
        "sample_tokens_str = [tokenizer.id_to_token(token) for token in sample_tokens_int]\n",
        "\n",
        "print(\"words        :\", sample_words.tolist())\n",
        "print(\"tokens (str) :\", sample_tokens_str)\n",
        "print(\"tokens (int) :\", sample_tokens_int)\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:08.999835Z",
          "iopub.execute_input": "2024-02-16T07:40:09.000136Z",
          "iopub.status.idle": "2024-02-16T07:40:10.175232Z",
          "shell.execute_reply.started": "2024-02-16T07:40:09.00011Z",
          "shell.execute_reply": "2024-02-16T07:40:10.17428Z"
        },
        "trusted": true,
        "id": "qWYJHqZk82eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessor in Action\n",
        "\n",
        "Even though we converted string inputs to integer tokens with Tokenizer, we are not done yet. We need to add special tokens like `[CLS]`, `[SEP]`, `[PAD]`. This is wehere `Preprocessing` layer comes into the picture. In this notebook, we will use `MultiSegmentPacker` layer. Let's see it action."
      ],
      "metadata": {
        "id": "kBtt-91l82eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sample_tokens_int = packer(np.array(sample_tokens_int))[0].tolist()\n",
        "padded_sample_tokens_str = [\n",
        "    tokenizer.id_to_token(token) for token in padded_sample_tokens_int\n",
        "]\n",
        "\n",
        "print(\"tokens (str)        :\", sample_tokens_str)\n",
        "print(\"padded tokens (str) :\", padded_sample_tokens_str, \"\\n\")\n",
        "\n",
        "print(\"tokens (int)        :\", sample_tokens_int)\n",
        "print(\"padded tokens (int) :\", padded_sample_tokens_int)\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:10.179401Z",
          "iopub.execute_input": "2024-02-16T07:40:10.179715Z",
          "iopub.status.idle": "2024-02-16T07:40:10.271891Z",
          "shell.execute_reply.started": "2024-02-16T07:40:10.179688Z",
          "shell.execute_reply": "2024-02-16T07:40:10.270913Z"
        },
        "trusted": true,
        "id": "nrH3WsdB82eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¥£ | Data Processing\n",
        "\n",
        "One of the key factors that sets Token Classification apart from Text Classification is the data processing part. Unlike text classification, where we simply send our tokenized text to the model, in token classification, we have to apply more processing before sending it to the model. For example, when the `tokenizer` creates multiple tokens for single word or the `processing` layer adds special tokens `[CLS]`, `[SEP]`, and `[PAD]`, they create a mismatch between the input and labels. Thus, a single word corresponding to a single label may now be split into two tokens. We need to realign the tokens labels with word labels by:\n",
        "\n",
        "- Mapping tokens label to their corresponding word label using `token_ids`.\n",
        "- Assigning the label `-100` to special tokens `[CLS]`, `[SEP]` and `[PAD]` to disregard them in the `CrossEntropy` loss calculation.\n",
        "- Labeling only the first token of each word and assigning `-100` to other tokens belonging to the same word.\n",
        "\n",
        "Specifically, the following cell contains the following functions:\n",
        "- `process_data()` - prepares input, label, and token_ids\n",
        "    - `get_tokens()` - creates input tokens ans padding masks from string words\n",
        "    - `get_token_ids()` - generates token ids for aligning tokens and labels\n",
        "    - `get_token_labels()` - realigns token labels and adds padding (`-100`) to match input\n",
        "    - `process_token_ids()` - adds padding (`-1`) to token ids to match input"
      ],
      "metadata": {
        "id": "FsHAULWp82eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(words, seq_len, packer):\n",
        "    # Tokenize input\n",
        "    token_words = tf.expand_dims(\n",
        "        tokenizer(words), axis=-1\n",
        "    )  # ex: (words) [\"It's\", \"a\", \"cat\"] ->  (token_words) [[1, 2], [3], [4]]\n",
        "    tokens = tf.reshape(\n",
        "        token_words, [-1]\n",
        "    )  # ex: (token_words) [[1, 2], [3], [4]] -> (tokens) [1, 2, 3, 4]\n",
        "    # Pad tokens\n",
        "    tokens = packer(tokens)[0][:seq_len]\n",
        "    inputs = {\"token_ids\": tokens, \"padding_mask\": tokens != 0}\n",
        "    return inputs, tokens, token_words\n",
        "\n",
        "\n",
        "def get_token_ids(token_words):\n",
        "    # Get word indices\n",
        "    word_ids = tf.range(tf.shape(token_words)[0])\n",
        "    # Get size of each word\n",
        "    word_size = tf.reshape(tf.map_fn(lambda word: tf.shape(word)[0:1], token_words), [-1])\n",
        "    # Repeat word_id with size of word to get token_id\n",
        "    token_ids = tf.repeat(word_ids, word_size)\n",
        "    return token_ids\n",
        "\n",
        "\n",
        "def get_token_labels(word_labels, token_ids, seq_len):\n",
        "    # Create token_labels from word_labels ->  alignment\n",
        "    token_labels = tf.gather(word_labels, token_ids)\n",
        "    # Only label the first token of a given word and assign -100 to others\n",
        "    mask = tf.concat([[True], token_ids[1:] != token_ids[:-1]], axis=0)\n",
        "    token_labels = tf.where(mask, token_labels, -100)\n",
        "    # Truncate to max sequence length\n",
        "    token_labels = token_labels[: seq_len - 2]  # -2 for special tokens ([CLS], [SEP])\n",
        "    # Pad token_labels to align with tokens (use -100 to pad for loss/metric ignore)\n",
        "    pad_start = 1  # for [CLS] token\n",
        "    pad_end = seq_len - tf.shape(token_labels)[0] - 1  # for [SEP] and [PAD] tokens\n",
        "    token_labels = tf.pad(token_labels, [[pad_start, pad_end]], constant_values=-100)\n",
        "    return token_labels\n",
        "\n",
        "\n",
        "def process_token_ids(token_ids, seq_len):\n",
        "    # Truncate to max sequence length\n",
        "    token_ids = token_ids[: seq_len - 2]  # -2 for special tokens ([CLS], [SEP])\n",
        "    # Pad token_ids to align with tokens (use -1 to pad for later identification)\n",
        "    pad_start = 1  # [CLS] token\n",
        "    pad_end = seq_len - tf.shape(token_ids)[0] - 1  # [SEP] and [PAD] tokens\n",
        "    token_ids = tf.pad(token_ids, [[pad_start, pad_end]], constant_values=-1)\n",
        "    return token_ids\n",
        "\n",
        "\n",
        "def process_data(seq_len=720, has_label=True, return_ids=False):\n",
        "    # To add spetical tokens: [CLS], [SEP], [PAD]\n",
        "    packer = keras_nlp.layers.MultiSegmentPacker(\n",
        "        start_value=tokenizer.cls_token_id,\n",
        "        end_value=tokenizer.sep_token_id,\n",
        "        sequence_length=seq_len,\n",
        "    )\n",
        "\n",
        "    def process(x):\n",
        "        # Generate inputs from tokens\n",
        "        inputs, tokens, words_int = get_tokens(x[\"words\"], seq_len, packer)\n",
        "        # Generate token_ids for maping tokens to words\n",
        "        token_ids = get_token_ids(words_int)\n",
        "        if has_label:\n",
        "            # Generate token_labels from word_labels\n",
        "            token_labels = get_token_labels(x[\"labels\"], token_ids, seq_len)\n",
        "            return inputs, token_labels\n",
        "        elif return_ids:\n",
        "            # Pad token_ids to align with tokens\n",
        "            token_ids = process_token_ids(token_ids, seq_len)\n",
        "            return token_ids\n",
        "        else:\n",
        "            return inputs\n",
        "\n",
        "    return process"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:10.273112Z",
          "iopub.execute_input": "2024-02-16T07:40:10.27347Z",
          "iopub.status.idle": "2024-02-16T07:40:10.28844Z",
          "shell.execute_reply.started": "2024-02-16T07:40:10.273438Z",
          "shell.execute_reply": "2024-02-16T07:40:10.287574Z"
        },
        "trusted": true,
        "id": "siqZ0F8Y82eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš | Dataloader\n",
        "\n",
        "The code below sets up a data flow pipeline using `tf.data.Dataset` for data processing. Notable aspects of `tf.data` include its ability to simplify pipeline construction and represent components in sequences. To learn more about `tf.data`, refer to this [documentation](https://www.tensorflow.org/guide/data).\n",
        "\n",
        "> **Note**: We have used `ragged` tensor as each row has text with different sizes."
      ],
      "metadata": {
        "id": "HcO6YM4_82eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(words, labels=None, return_ids=False, batch_size=4,\n",
        "                  seq_len=512, shuffle=False, cache=True, drop_remainder=True):\n",
        "    AUTO = tf.data.AUTOTUNE\n",
        "\n",
        "    slices = {\"words\": tf.ragged.constant(words)}\n",
        "    if labels is not None:\n",
        "        slices.update({\"labels\": tf.ragged.constant(labels)})\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
        "    ds = ds.map(process_data(seq_len=seq_len,\n",
        "                             has_label=labels is not None,\n",
        "                             return_ids=return_ids), num_parallel_calls=AUTO) # apply processing\n",
        "    ds = ds.cache() if cache else ds  # cache dataset\n",
        "    if shuffle: # shuffle dataset\n",
        "        ds = ds.shuffle(1024, seed=CFG.seed)\n",
        "        opt = tf.data.Options()\n",
        "        opt.experimental_deterministic = False\n",
        "        ds = ds.with_options(opt)\n",
        "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)  # batch dataset\n",
        "    ds = ds.prefetch(AUTO)  # prefetch next batch\n",
        "    return ds"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:10.289907Z",
          "iopub.execute_input": "2024-02-16T07:40:10.290301Z",
          "iopub.status.idle": "2024-02-16T07:40:10.304393Z",
          "shell.execute_reply.started": "2024-02-16T07:40:10.290267Z",
          "shell.execute_reply": "2024-02-16T07:40:10.303588Z"
        },
        "trusted": true,
        "id": "Lx8w0dfU82eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Train & Valid Dataloader\n",
        "\n",
        "In the following code, we'll create **train** and **valid** data loaders."
      ],
      "metadata": {
        "id": "gtr_1WEQ82eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = build_dataset(train_words, train_labels,  batch_size=CFG.train_batch_size,\n",
        "                         seq_len=CFG.train_seq_len, shuffle=True)\n",
        "\n",
        "valid_ds = build_dataset(valid_words, valid_labels, batch_size=CFG.train_batch_size,\n",
        "                         seq_len=CFG.train_seq_len, shuffle=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:10.305368Z",
          "iopub.execute_input": "2024-02-16T07:40:10.305617Z",
          "iopub.status.idle": "2024-02-16T07:40:42.695659Z",
          "shell.execute_reply.started": "2024-02-16T07:40:10.305596Z",
          "shell.execute_reply": "2024-02-16T07:40:42.694869Z"
        },
        "trusted": true,
        "id": "2p7nbhbu82eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Check\n",
        "\n",
        "Let's check a batch of samples and their associated labels from the dataset."
      ],
      "metadata": {
        "id": "3d1r4niI82eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp, tar = next(iter(valid_ds))\n",
        "print(\"# Input:\\n\",inp); print(\"\\n# Labels:\\n\",tar)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:42.696806Z",
          "iopub.execute_input": "2024-02-16T07:40:42.697084Z",
          "iopub.status.idle": "2024-02-16T07:40:43.304332Z",
          "shell.execute_reply.started": "2024-02-16T07:40:42.69706Z",
          "shell.execute_reply": "2024-02-16T07:40:43.303303Z"
        },
        "trusted": true,
        "id": "zel3Xb6j82eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ” | Loss & Metric"
      ],
      "metadata": {
        "id": "5TEBlGyb82et"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss: CrossEntropy\n",
        "\n",
        "To optimize our model we will use `CrossEntropy` loss, also known as log loss. It is defined as:\n",
        "\n",
        "$$\n",
        "\\text{CrossEntropy} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $N$ is the number of samples.\n",
        "- $y_i$ is the true label of the $i^{th}$ sample.\n",
        "- $\\hat{y}_i$ is the predicted probability of the $i^{th}$ sample being in the positive class.\n",
        "\n",
        "> **Note**: We will not compute loss for `ignore_class` which indicates special tokens (`[CLS]`, `[SEP]`, `[PAD]`) or intermediate token of a word."
      ],
      "metadata": {
        "id": "qyJBw0yL82et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropy(keras.losses.SparseCategoricalCrossentropy):\n",
        "    def __init__(self, ignore_class=-100, reduction=None, **args):\n",
        "        super().__init__(reduction=reduction, **args)\n",
        "        self.ignore_class = ignore_class\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = ops.reshape(y_true, [-1])\n",
        "        y_pred = ops.reshape(y_pred, [-1, CFG.num_labels])\n",
        "        loss = super().call(y_true, y_pred)\n",
        "        if self.ignore_class is not None:\n",
        "            valid_mask = ops.not_equal(\n",
        "                y_true, ops.cast(self.ignore_class, y_pred.dtype)\n",
        "            )\n",
        "            loss = ops.where(valid_mask, loss, 0.0)\n",
        "            loss = ops.sum(loss)\n",
        "            loss /= ops.maximum(ops.sum(ops.cast(valid_mask, loss.dtype)), 1)\n",
        "        else:\n",
        "            loss = ops.mean(loss)\n",
        "        return loss\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:43.305515Z",
          "iopub.execute_input": "2024-02-16T07:40:43.305837Z",
          "iopub.status.idle": "2024-02-16T07:40:43.313651Z",
          "shell.execute_reply.started": "2024-02-16T07:40:43.305812Z",
          "shell.execute_reply": "2024-02-16T07:40:43.31277Z"
        },
        "trusted": true,
        "id": "3eJF2FMt82et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metric: FBetaScore ($\\beta = 5$)\n",
        "\n",
        "The competition metric is $F^\\beta$, which combines precision and recall, weighted by a parameter $\\beta = 5$. It is defined as:\n",
        "\n",
        "$$\n",
        "\\text{FBetaScore} = (1 + \\beta^2) \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\beta^2 \\times \\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- Precision $= \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$\n",
        "- Recall $= \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$\n",
        "- $\\beta$ controls the weighting between precision and recall. As in this competition, $\\beta = 5$, it means more weight is given to recall. In other words, **metric will penalize more, if a positive token is classified as negative**.\n",
        "\n",
        "> **Note${}^1$**: The competition will use `micro` averaging for the `FBetaScore`, considering total counts across all classes, which is influenced by class imbalances. The `macro` averaging treats each class equally, regardless of frequency. Organizers may want models that perform well on predicting more common PII tags.\n",
        "\n",
        "> **Note${}^2$**: We will not compute the metric for `ignore_classes`, which indicates special tokens (`[CLS]`, `[SEP]`, `[PAD]`) or non-start tokens of a word or `O` (Outer) labels."
      ],
      "metadata": {
        "id": "Vd_l4ros82et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FBetaScore(keras.metrics.FBetaScore):\n",
        "    def __init__(self, ignore_classes=[-100, 12], average=\"micro\", beta=5.0,\n",
        "                 name=\"f5_score\", **args):\n",
        "        super().__init__(beta=beta, average=average, name=name, **args)\n",
        "        self.ignore_classes = ignore_classes or []\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = ops.convert_to_tensor(y_true, dtype=self.dtype)\n",
        "        y_pred = ops.convert_to_tensor(y_pred, dtype=self.dtype)\n",
        "\n",
        "        y_true = ops.reshape(y_true, [-1])\n",
        "        y_pred = ops.reshape(y_pred, [-1, CFG.num_labels])\n",
        "\n",
        "        valid_mask = ops.ones_like(y_true, dtype=self.dtype)\n",
        "        if self.ignore_classes:\n",
        "            for ignore_class in self.ignore_classes:\n",
        "                valid_mask &= ops.not_equal(y_true, ops.cast(ignore_class, y_pred.dtype))\n",
        "        valid_mask = ops.expand_dims(valid_mask, axis=-1)\n",
        "\n",
        "        y_true = ops.one_hot(y_true, CFG.num_labels)\n",
        "\n",
        "        if not self._built:\n",
        "            self._build(y_true.shape, y_pred.shape)\n",
        "\n",
        "        threshold = ops.max(y_pred, axis=-1, keepdims=True)\n",
        "        y_pred = ops.logical_and(\n",
        "            y_pred >= threshold, ops.abs(y_pred) > 1e-9\n",
        "        )\n",
        "\n",
        "        y_pred = ops.cast(y_pred, dtype=self.dtype)\n",
        "        y_true = ops.cast(y_true, dtype=self.dtype)\n",
        "\n",
        "        tp = ops.sum(y_pred * y_true * valid_mask, self.axis)\n",
        "        fp = ops.sum(y_pred * (1 - y_true) * valid_mask, self.axis)\n",
        "        fn = ops.sum((1 - y_pred) * y_true * valid_mask, self.axis)\n",
        "\n",
        "        self.true_positives.assign_add(tp)\n",
        "        self.false_positives.assign_add(fp)\n",
        "        self.false_negatives.assign_add(fn)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:43.314849Z",
          "iopub.execute_input": "2024-02-16T07:40:43.315102Z",
          "iopub.status.idle": "2024-02-16T07:40:43.331715Z",
          "shell.execute_reply.started": "2024-02-16T07:40:43.31508Z",
          "shell.execute_reply": "2024-02-16T07:40:43.330835Z"
        },
        "trusted": true,
        "id": "c5BKXN5D82et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤– | Modeling\n",
        "\n",
        "In this notebook, we will use the `DebertaV3` backbone from KerasNLP's pretrained models to extract features of tokens and employ `Dense` layers for token-level classification. Unlike Text Classification, transformer outputs are not pooled; instead, a `Dense` layer is applied to the outputs to obtain predictions.\n",
        "\n",
        "To clarify, the output of the transformer model is a 3D tensor of shape $(batch\\_size, seq\\_len, feat\\_dim)$, where only the $feat\\_dim$ is changed, while the others remain the same. Subsequently, the `Dense` (or `Linear`) layer maps the `feat_dim` to `num_labels` and then applies a `softmax` activation to get the final prediction.\n",
        "\n",
        "To explore other backbones, simply modify the `preset` in the `CFG` (config). A list of available pretrained backbones can be found on the [KerasNLP website](https://keras.io/api/keras_nlp/models/).\n",
        "\n",
        "> **Note:** The output `dtype` of the final activation is manually set to `float32` to facilitate `mixed_precision`.\n",
        "\n",
        "<u>Food for thought</u>:\n",
        "1. Some may wonder why the input to the `Dense` layer is 3D `(batch_size, d0, d1)` instead of the traditional 2D `(batch_size, d0)`. You can check [Hint: you can check this page](https://keras.io/api/layers/core_layers/dense/).\n",
        "2. We are training our model with sequence of `1024` length, however we are doing inference with sequence of `2000` length. What is happening here?"
      ],
      "metadata": {
        "id": "eCCQOJ0682et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Token Classification model\n",
        "backbone = keras_nlp.models.DebertaV3Backbone.from_preset(\n",
        "    CFG.preset,\n",
        ")\n",
        "out = backbone.output\n",
        "out = keras.layers.Dense(CFG.num_labels, name=\"logits\")(out)\n",
        "out = keras.layers.Activation(\"softmax\", dtype=\"float32\", name=\"prediction\")(out)\n",
        "model = keras.models.Model(backbone.input, out)\n",
        "\n",
        "# Compile model for optimizer, loss and metric\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=2e-5),\n",
        "    loss=CrossEntropy(),\n",
        "    metrics=[FBetaScore()],\n",
        ")\n",
        "\n",
        "# Summary of the model architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:43.332842Z",
          "iopub.execute_input": "2024-02-16T07:40:43.333175Z",
          "iopub.status.idle": "2024-02-16T07:40:54.593423Z",
          "shell.execute_reply.started": "2024-02-16T07:40:43.333143Z",
          "shell.execute_reply": "2024-02-16T07:40:54.592517Z"
        },
        "trusted": true,
        "id": "WLifSOx882et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âš“ | LR Schedule\n",
        "\n",
        "A well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation."
      ],
      "metadata": {
        "id": "Vb1q0z-882eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n",
        "    lr_start, lr_max, lr_min = 6e-6, 2.5e-6 * batch_size, 1e-6\n",
        "    lr_ramp_ep, lr_sus_ep, lr_decay = 3, 0, 0.75\n",
        "\n",
        "    def lrfn(epoch):  # Learning rate update function\n",
        "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
        "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
        "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
        "        elif mode == 'cos':\n",
        "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n",
        "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
        "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
        "        return lr\n",
        "\n",
        "    if plot:  # Plot lr curve if plot is True\n",
        "        fig = px.line(x=np.arange(epochs),\n",
        "                      y=[lrfn(epoch) for epoch in np.arange(epochs)],\n",
        "                      title='LR Scheduler',\n",
        "                      markers=True,\n",
        "                      labels={'x': 'epoch', 'y': 'lr'})\n",
        "        fig.update_layout(\n",
        "            yaxis = dict(\n",
        "                showexponent = 'all',\n",
        "                exponentformat = 'e'\n",
        "            )\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:54.594578Z",
          "iopub.execute_input": "2024-02-16T07:40:54.594857Z",
          "iopub.status.idle": "2024-02-16T07:40:54.605291Z",
          "shell.execute_reply.started": "2024-02-16T07:40:54.594833Z",
          "shell.execute_reply": "2024-02-16T07:40:54.60435Z"
        },
        "trusted": true,
        "id": "n2vePu1n82eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_cb = get_lr_callback(CFG.train_batch_size, mode=CFG.lr_mode, plot=True)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:54.606471Z",
          "iopub.execute_input": "2024-02-16T07:40:54.606765Z",
          "iopub.status.idle": "2024-02-16T07:40:55.937966Z",
          "shell.execute_reply.started": "2024-02-16T07:40:54.606742Z",
          "shell.execute_reply": "2024-02-16T07:40:55.937016Z"
        },
        "trusted": true,
        "id": "nx3vBPOQ82eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš‚ | Training"
      ],
      "metadata": {
        "id": "4m_zUS5Y82eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if CFG.train:\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=valid_ds,\n",
        "        epochs=CFG.epochs,\n",
        "        callbacks=[lr_cb],\n",
        "        verbose=1,\n",
        "    )\n",
        "else:\n",
        "    model.load_weights(\"/kaggle/input/pii-data-detection-ckpt-ds/model.weights.h5\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-16T07:40:55.939256Z",
          "iopub.execute_input": "2024-02-16T07:40:55.939628Z"
        },
        "trusted": true,
        "id": "j69zWYYK82eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though this notebook does both training and inference, let's store the **weights** of the trained model on disk in case we may need it later. Also, if you need more time for inference, you can create separate notebooks for training and inference.\n",
        "\n",
        "> **Note**: The filename of the weights should end in `.weights.h5`"
      ],
      "metadata": {
        "id": "uTnhcEQj82eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"model.weights.h5\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "UtWQ4vFO82eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”¬ | Evaluation\n",
        "\n",
        "We have trained and validated our model on a `1024` sequence length; however, we will be making inference using a `2000` sequence length. Thus, it is important to check how our model performs with `2000` sequence length inputs."
      ],
      "metadata": {
        "id": "NdtOq3Dt82eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Validation dataloader with \"infer_seq_len\"\n",
        "valid_ds = build_dataset(valid_words, valid_labels, return_ids=False, batch_size=CFG.infer_batch_size,\n",
        "                        seq_len=CFG.infer_seq_len, shuffle=False, cache=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "aBzRpNoa82eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "model.evaluate(valid_ds, return_dict=True, verbose=0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NYhmXSB882eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§ª | Prediction"
      ],
      "metadata": {
        "id": "qcT75wq182eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Test Dataloader"
      ],
      "metadata": {
        "id": "_b00i44H82eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data\n",
        "test_data = json.load(open(f\"{BASE_PATH}/test.json\"))\n",
        "\n",
        "# Ensure number of samples is divisble by number of devices\n",
        "need_samples  = len(devices) - len(test_data) % len(devices)\n",
        "for _ in range(need_samples):\n",
        "    test_data.append(test_data[-1]) # repeat the last sample\n",
        "\n",
        "# Initialize empty arrays\n",
        "test_words = np.empty(len(test_data), dtype=object)\n",
        "test_docs = np.empty(len(test_data), dtype=np.int32)\n",
        "\n",
        "# Fill the arrays\n",
        "for i, x in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "    test_words[i] = np.array(x[\"tokens\"])\n",
        "    test_docs[i] = x[\"document\"]\n",
        "\n",
        "# Get token ids\n",
        "id_ds = build_dataset(test_words, return_ids=True, batch_size=len(test_words),\n",
        "                        seq_len=CFG.infer_seq_len, shuffle=False, cache=False, drop_remainder=False)\n",
        "test_token_ids = ops.convert_to_numpy([ids for ids in iter(id_ds)][0])\n",
        "\n",
        "# Build test dataloader\n",
        "test_ds = build_dataset(test_words, return_ids=False, batch_size=CFG.infer_batch_size,\n",
        "                        seq_len=CFG.infer_seq_len, shuffle=False, cache=False, drop_remainder=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "yRbidOjD82eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "lDjybstA82eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do inference\n",
        "test_preds = model.predict(test_ds, verbose=1)\n",
        "\n",
        "# Convert probabilities to class labels via max confidence\n",
        "test_preds = np.argmax(test_preds, axis=-1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "-lnaqU_d82eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Extra Samples\n",
        "\n",
        "We need to remove the extra samples we added to the test data to ensure the number of samples is divisible by the number of devices."
      ],
      "metadata": {
        "id": "E5LOveL_82eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_docs = test_docs[:-need_samples]\n",
        "test_token_ids = test_token_ids[:-need_samples]\n",
        "test_preds = test_preds[:-need_samples]\n",
        "test_words = test_words[:-need_samples]"
      ],
      "metadata": {
        "trusted": true,
        "id": "e3GTHnmY82eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§¹ | Post-Processing\n",
        "\n",
        "The following code processes the prediction to filter out unwanted parts. Specifically, it does the following:\n",
        "\n",
        "1. It filters out any tokens of a word that are not at the start (refer to the [ðŸ¥£ | Data Processing](https://www.kaggle.com/code/awsaf49/pii-data-detection-kerasnlp-starter-notebook#%F0%9F%A5%A3-%7C-Data-Processing) section for more details).\n",
        "2. It removes samples labeled as `O` (BIO format), as the submission file requires only non-`O` samples.\n",
        "3. It ignores predictions for special tokens like `[CLS]`, `[SEP]`, and `[PAD]`.\n",
        "\n",
        "> **Note**: A unique feature of following post-processing is that it uses numpy vectorized operations to filter out predictions, making it very fast and efficient."
      ],
      "metadata": {
        "id": "3VPT5U4R82eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_list = []\n",
        "token_id_list = []\n",
        "label_id_list = []\n",
        "token_list = []\n",
        "\n",
        "for doc, token_ids, preds, tokens in tqdm(\n",
        "    zip(test_docs, test_token_ids, test_preds, test_words), total=len(test_words)\n",
        "):\n",
        "    # Create mask for filtering\n",
        "    mask1 = np.concatenate(([True], token_ids[1:] != token_ids[:-1])) # ignore non-start tokens of a word\n",
        "    mask2 = (preds != 12) # ignore `O` (BIO format) label -> 12 (integer format) label\n",
        "    mask3 = (token_ids != -1)  # ignore [CLS], [SEP], and [PAD] tokens\n",
        "    mask = (mask1 & mask2 & mask3) # merge filters\n",
        "\n",
        "    # Apply filter\n",
        "    token_ids = token_ids[mask]\n",
        "    preds = preds[mask]\n",
        "\n",
        "     # Store prediction if number of tokens is not zero\n",
        "    if len(token_ids):\n",
        "        token_list.extend(tokens[token_ids])\n",
        "        document_list.extend([doc] * len(token_ids))\n",
        "        token_id_list.extend(token_ids)\n",
        "        label_id_list.extend(preds)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tQ30BhEI82eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“© | Submission\n",
        "\n",
        "Let's build a dataframe from the predictions which will help us visually check if our model is predicting correctly or not. We also have to map **integer** labels to **string** BIO format labels."
      ],
      "metadata": {
        "id": "qomQxZpv82ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_df = pd.DataFrame(\n",
        "    {\n",
        "        \"document\": document_list,\n",
        "        \"token\": token_id_list,\n",
        "        \"label_id\": label_id_list,\n",
        "        \"token_string\": token_list,\n",
        "    }\n",
        ")\n",
        "pred_df = pred_df.rename_axis(\"row_id\").reset_index() # add `row_id` column\n",
        "pred_df[\"label\"] = pred_df.label_id.map(CFG.id2label) # map integer label to BIO format label\n",
        "pred_df.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "_Konil7N82ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In `submission.csv` we are excluding `token_string` and `label_id` from the columns as they are not part of submission file."
      ],
      "metadata": {
        "id": "uFsSEEH282ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df = pred_df.drop(columns=[\"token_string\", \"label_id\"]) # remove extra columns\n",
        "sub_df.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "p3QgufBR82ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”— | Reference\n",
        "* [Detect Fake Text: KerasNLP [TF/Torch/JAX][Train]](https://www.kaggle.com/code/awsaf49/detect-fake-text-kerasnlp-tf-torch-jax-train)\n",
        "* [Token classification](https://huggingface.co/docs/transformers/en/tasks/token_classification)\n",
        "* [transformer ner baseline [lb 0.854]](https://www.kaggle.com/code/nbroad/transformer-ner-baseline-lb-0-854)"
      ],
      "metadata": {
        "id": "V472xuOc82ev"
      }
    }
  ]
}